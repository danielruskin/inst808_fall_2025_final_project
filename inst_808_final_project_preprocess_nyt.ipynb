{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNC91jqawTVpAI6HCvg5bRA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielruskin/inst808_fall_2025_final_project/blob/main/inst_808_final_project_preprocess_nyt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Code for preprocessing OCR text from the NYT ads"
      ],
      "metadata": {
        "id": "InL-_qD_-bqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional code block for mounting your Google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykEj9HnNsimR",
        "outputId": "821b3601-d2fb-4a6f-8b28-6c3d467152bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "from datetime import datetime\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "x8tp3j3eT8eY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load data"
      ],
      "metadata": {
        "id": "GMuRrULpwioj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/drive/My Drive/Courses/INST808'\n",
        "file_path = os.path.join(data_path, 'nyt_ads_modified.tsv')\n",
        "df_nyt = pd.read_csv(file_path, sep='\\t')"
      ],
      "metadata": {
        "id": "NQX7YlydTadt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocess text"
      ],
      "metadata": {
        "id": "HzxsCUo1wjYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for now, ignore rows with OCR tab issue\n",
        "df_nyt = df_nyt.drop(df_nyt.index[[4, 10, 16, 18, 19, 28, 32, 33, 39, 60, 63, 65, 69]]).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "YkFF7vC0-uGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop na's. Should leave you with just two columns\n",
        "df_nyt = df_nyt.dropna(axis=1)"
      ],
      "metadata": {
        "id": "GTWzu1t7Aya4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split for strings before and after dates in the text output, such that only dates are returned\n",
        "first_split = df_nyt['text'].str.split('; ProQuest', expand=True)[0]\n",
        "second_split = first_split.str.split(';')\n",
        "dates = [splits[-1].strip() for splits in second_split] # also strip whitespace at beginning and end to put in proper datetime form\n",
        "dates = [datetime.strptime(date, \"%b %d, %Y\") for date in dates] # convert to datetime\n",
        "df_nyt['date'] = dates"
      ],
      "metadata": {
        "id": "ssDNCCDMNLFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = df_nyt['text_modified']\n",
        "\n",
        "# Handle newlines. Ignore paragraphs for now\n",
        "\n",
        "#texts = [text.replace('\\\\n\\\\n', ' PARAGRAPH') for text in texts] #  indicate paragraphs\n",
        "texts = [text.replace('-\\\\n','') for text in texts] # dashes followed by newline characters are just one word each\n",
        "texts = [text.replace('\\\\n',' ') for text in texts] # all other newline characters should be replcaed by spaces\n",
        "#texts = [text.replace('PARAGRAPH', '\\\\n\\\\n') for text in texts] # replace paragraphs with newlines\n",
        "texts = [re.sub(r'\\s+', ' ', text) for text in texts] # replace all multiple whitespace with just one whitespace\n",
        "df_nyt['text_modified_2'] = texts"
      ],
      "metadata": {
        "id": "DkOd3LKKdjVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: I cleared the print output of this when pushing to github because it is very long\n",
        "for i, row in df_nyt.iterrows():\n",
        "  print(f'TEXT {i}')\n",
        "  print(row['text_modified_2'])"
      ],
      "metadata": {
        "id": "1pdx-6khm4Z5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenize and Lemmatize"
      ],
      "metadata": {
        "id": "W_mkmGgCwyEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSrwzgmW03Fc",
        "outputId": "93dd776c-e9ae-4515-8177-92a9f7539690"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.3.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_rule_based(text):\n",
        "  # Return a list of lemmas as processed by SpaCy\n",
        "  doc = nlp(text)\n",
        "  # grab result of lemmatization for each token, as long as token is not stop token or punctuation or 1 char or a number\n",
        "  # also keep just the lowercase version\n",
        "  tokens = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct and not (len(token) == 1) and not token.like_num]\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "iVBgZLpCnmNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_nyt['spacy'] = df_nyt['text_modified_2'].apply(tokenize_rule_based) # tokenize and lemmatize using spacy"
      ],
      "metadata": {
        "id": "kBMpykwinuPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combine bi-grams that occur often"
      ],
      "metadata": {
        "id": "s1HdrbOf8dQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2DTKSvH6sKr",
        "outputId": "fea7220d-3479-4a6b-ee07-b82b33ac851b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.phrases import Phrases, Phraser"
      ],
      "metadata": {
        "id": "pR-haTGB4dkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combine 2-word phrases (e.g., \"climate change\"). They must appear at least 10 times. Use default threshold from gensim\n",
        "bigram_model = Phrases(df_nyt['spacy'], min_count=10, threshold=10)\n",
        "bigram_phraser = Phraser(bigram_model)\n",
        "df_nyt['final_tokens'] = [bigram_phraser[tokens] for tokens in df_nyt['spacy']]\n",
        "# See example of climate change being put into one phrase below:\n",
        "print(df_nyt['final_tokens'].iloc[20])\n",
        "print(df_nyt['spacy'].iloc[20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dyJaclH4PAr",
        "outputId": "76e18f37-8a28-4812-9447-6187700f7ef3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['weather', 'climate', 'debate', 'climate_change', 'understandable', 'tendency', 'use', 'recent', 'weather', 'event', 'draw', 'conclusion', 'global_warming', 'weather', 'climate', 'climate', 'far', 'complex', 'know', 'weather', 'clear', 'climate', 'region', 'climate', 'define', 'prevail', 'behavior', 'weather', 'include', 'variability', 'decade', 'weather', 'ordinarily', 'consider', 'establish', 'average', 'condition', 'variability', 'climate', 'recent', 'record', 'cold', 'weather', 'northeast', 'u.s.', 'indicate', 'cool', 'clitate', 'year', 'record', 'summer', 'heat', 'europe', 'confirm', 'warm', 'world', 'geological', 'evidence', 'indicate', 'earth', 'climate', 'hds', 'vary', 'continuously', 'warm', 'cool', 'change', 'earth', 'factor', 'diverse', 'variation', 'sunlight', 'earth', 'magnetic', 'field', 'asteroid', 'impact', 'sun', 'moon', 'earth', 'orbital', 'interaction', 'cosmic', 'ray', 'flux', 'continental', 'drift', 'fluctuation', 'ea', 'level', 'volcanic', 'eruption', 'change', 'biosphere', 'massive', 'ebb', 'flow', 'continental', 'glacier', 'significantly', 'influence', 'climate_change', 'feature', 'affect', 'recent', 'ice', 'age', 'factor', 'greenhouse_gas', 'concentration', 'change', 'reason', 'remain', 'unclear', 'evidence', 'suggest', 'shift', 'flow', 'dust', 'nutrient', 'land', 'ocean', 'significantly', 'alter', 'exchange', 'carbon_dioxide', 'changes', 'feature', 'affect', 'air', 'ocean', 'observation', 'theory', 'indicate', 'weather', 'important', 'aspect', 'climate', 'instance', 'nino', 'event', 'behave', 'chaotic', 'fashion', 'allow', 'definitive', 'longterm', 'prediction', 'fluctuation', 'produce', 'significant', 'natural', 'climate', 'variability', 'example', 'past', 'year', 'historical', 'account', 'scientific', 'datum', 'evidence', 'medieval', 'warm', 'period', 'follow', 'little', 'ice', 'age', 'face', 'natural', 'variability', 'complexity', 'consequence', 'change', 'single', 'factor', 'exampie', 'greenhouse_gas', 'readily', 'isolate', 'prediction', 'difficult', 'geological', 'historical', 'record', 'clear', 'need', 'account', 'natural', 'climate', 'variability', 'integrated', 'response', 'sntire', 'climate', 'system', 'decade', 'climate', 'research', 'great', 'progress', 'particular', 'research', 'highlight', 'risk', 'society', 'ecosystem', 'result', 'buildup', 'greenhouse_gas', 'time', 'scientific', 'uncertainty', 'continue', 'limit', 'ability', 'objective', 'quantitative', 'determination', 'human', 'role', 'recent', 'climate_change', 'degree', 'consequence', 'future', 'change', 'reinforce', 'view', 'country', 'society', 'work', 'find', 'acceptable', 'approach', 'address', 'climate_change', 'continue', 'promote', 'global', 'prosperity', 'ongoing', 'need', 'support', 'scientific', 'research', 'inform', 'decision', 'guide', 'policy']\n",
            "['weather', 'climate', 'debate', 'climate', 'change', 'understandable', 'tendency', 'use', 'recent', 'weather', 'event', 'draw', 'conclusion', 'global', 'warming', 'weather', 'climate', 'climate', 'far', 'complex', 'know', 'weather', 'clear', 'climate', 'region', 'climate', 'define', 'prevail', 'behavior', 'weather', 'include', 'variability', 'decade', 'weather', 'ordinarily', 'consider', 'establish', 'average', 'condition', 'variability', 'climate', 'recent', 'record', 'cold', 'weather', 'northeast', 'u.s.', 'indicate', 'cool', 'clitate', 'year', 'record', 'summer', 'heat', 'europe', 'confirm', 'warm', 'world', 'geological', 'evidence', 'indicate', 'earth', 'climate', 'hds', 'vary', 'continuously', 'warm', 'cool', 'change', 'earth', 'factor', 'diverse', 'variation', 'sunlight', 'earth', 'magnetic', 'field', 'asteroid', 'impact', 'sun', 'moon', 'earth', 'orbital', 'interaction', 'cosmic', 'ray', 'flux', 'continental', 'drift', 'fluctuation', 'ea', 'level', 'volcanic', 'eruption', 'change', 'biosphere', 'massive', 'ebb', 'flow', 'continental', 'glacier', 'significantly', 'influence', 'climate', 'change', 'feature', 'affect', 'recent', 'ice', 'age', 'factor', 'greenhouse', 'gas', 'concentration', 'change', 'reason', 'remain', 'unclear', 'evidence', 'suggest', 'shift', 'flow', 'dust', 'nutrient', 'land', 'ocean', 'significantly', 'alter', 'exchange', 'carbon', 'dioxide', 'changes', 'feature', 'affect', 'air', 'ocean', 'observation', 'theory', 'indicate', 'weather', 'important', 'aspect', 'climate', 'instance', 'nino', 'event', 'behave', 'chaotic', 'fashion', 'allow', 'definitive', 'longterm', 'prediction', 'fluctuation', 'produce', 'significant', 'natural', 'climate', 'variability', 'example', 'past', 'year', 'historical', 'account', 'scientific', 'datum', 'evidence', 'medieval', 'warm', 'period', 'follow', 'little', 'ice', 'age', 'face', 'natural', 'variability', 'complexity', 'consequence', 'change', 'single', 'factor', 'exampie', 'greenhouse', 'gas', 'readily', 'isolate', 'prediction', 'difficult', 'geological', 'historical', 'record', 'clear', 'need', 'account', 'natural', 'climate', 'variability', 'integrated', 'response', 'sntire', 'climate', 'system', 'decade', 'climate', 'research', 'great', 'progress', 'particular', 'research', 'highlight', 'risk', 'society', 'ecosystem', 'result', 'buildup', 'greenhouse', 'gas', 'time', 'scientific', 'uncertainty', 'continue', 'limit', 'ability', 'objective', 'quantitative', 'determination', 'human', 'role', 'recent', 'climate', 'change', 'degree', 'consequence', 'future', 'change', 'reinforce', 'view', 'country', 'society', 'work', 'find', 'acceptable', 'approach', 'address', 'climate', 'change', 'continue', 'promote', 'global', 'prosperity', 'ongoing', 'need', 'support', 'scientific', 'research', 'inform', 'decision', 'guide', 'policy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save preprocessed text to file"
      ],
      "metadata": {
        "id": "k9y7TFIgxIcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = os.path.join(data_path, 'nyt_ads_modified_preprocessed.tsv')\n",
        "df_nyt.to_csv(file_path, sep=\"\\t\", index=False)"
      ],
      "metadata": {
        "id": "uI2JxrqWX-R0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}